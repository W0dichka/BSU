1. Задание 1: Отбор признаков с использованием корреляционных матриц.
   Для начала, используйте датасет о винах из sklearn.datasets. Найти корреляционную матрицу для датасета и, дополнительно, визуализируйте ее с помощью seaborn. 
2. Задание 2: Отбор признаков на основе важности признаков в случайном лесе.
   Используйте тот же датасет и подгоните модель RandomForestRegressor к вашим данным, затем используйте атрибут feature_importances_ чтобы определить наиболее важные признаки.
3. Задание 3: Использование выбора признаков на основе p-value.
   Для этого задания можно использовать набор данных Boston Housing из sklearn.datasets.
4. Задание 4: Отбор признаков с помощь метода взаимной информации.
   Используйте любой датасет, с которым вам приятно работать. Установите библиотеку sklearn и примените функцию mutual_info_classif или mutual_info_regression для отбора признаков.
5. Задание 5: Используйте метод Recursive Feature Elimination на том же датасете.
   Постройте модель, например, линейную регрессию или SVM, и используйте RFE для выбора лучшего подмножества признаков.
6. Задание 6: Сравните различные методы отбора признаков. 
Примените каждый из методов отбора признаков к одному и тому же датасету, а затем сравните производительность моделей машинного обучения, обученных на этих различных подмножествах признаков.
Каждое задание должно включать в себя следующие шаги: загрузка и предварительная обработка данных, применение метода отбора признаков, обучение модели на выбранных признаках и оценка производительности модели. Для оценки модели могут быть использованы такие метрики, как accuracy для задач классификации и MSE для задач регрессии. 
Здесь можно найти больше датасетов для этих задач:
[UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php), 
[Kaggle Datasets](https://www.kaggle.com/datasets), 
[Google's Dataset Search](https://datasetsearch.research.google.com/)